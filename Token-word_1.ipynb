{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvGQYssHzkvTVfnZmUsl30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumata-arc/Probabilities_Surprisal/blob/main/Token-word_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# БЛОК 1: Excel -> единицы-ячейки + определение пунктуации\n",
        "import re\n",
        "import openpyxl\n",
        "\n",
        "PUNCT_CHARS = set(list(\".,!?;:…—–-()[]{}«»\\\"'`“”„”“/\\\\|\"))\n",
        "\n",
        "def is_punct_only_cell(s: str) -> bool:\n",
        "    if s is None:\n",
        "        return False\n",
        "    t = str(s).strip()\n",
        "    if t == \"\":\n",
        "        return False\n",
        "    return all((ch in PUNCT_CHARS) for ch in t)\n",
        "\n",
        "def extract_cell_units_from_sheet(ws, start_row=2, start_col=1):\n",
        "    units = []\n",
        "    u = 0\n",
        "    for col in range(start_col, ws.max_column + 1):\n",
        "        header = ws.cell(row=1, column=col).value\n",
        "        if header is None:\n",
        "            continue\n",
        "        for row in range(start_row, ws.max_row + 1):\n",
        "            v = ws.cell(row=row, column=col).value\n",
        "            if v is None:\n",
        "                break\n",
        "            s = str(v).strip()\n",
        "            if s == \"\":\n",
        "                break\n",
        "\n",
        "            units.append({\n",
        "                \"unit_index\": u,\n",
        "                \"text\": s,\n",
        "                \"is_punct\": is_punct_only_cell(s),\n",
        "                \"row\": row,\n",
        "                \"col\": col\n",
        "            })\n",
        "            u += 1\n",
        "    return units\n",
        "\n",
        "print(\"✅ Block 1 OK: functions loaded\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nVmjHh49Pys",
        "outputId": "65622087-aebe-4de6-dc6a-2cd8a6c20004"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Block 1 OK: functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# БЛОК 2: alignment токенов CSV к ячейкам-единицам\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "TOKENIZER_NAME_OR_PATH = \"Qwen/Qwen1.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, use_fast=True)\n",
        "\n",
        "def encode_unit_candidates(unit_text: str, is_first: bool, is_punct: bool):\n",
        "    \"\"\"\n",
        "    Возвращает варианты token_id для unit_text с разными префиксами.\n",
        "    Для пунктуации-ячейки сначала пробуем без пробела.\n",
        "    \"\"\"\n",
        "    t = str(unit_text)\n",
        "\n",
        "    prefixes = []\n",
        "    if is_first:\n",
        "        prefixes = [\"\"]  # первое — без пробела\n",
        "    else:\n",
        "        # если пунктуация-ячейка, чаще она \"прилипает\" без пробела: слово + \",\".\n",
        "        if is_punct:\n",
        "            prefixes = [\"\", \" \", \"\\n\"]\n",
        "        else:\n",
        "            prefixes = [\" \", \"\", \"\\n\"]\n",
        "\n",
        "    # формируем кандидаты\n",
        "    cands = []\n",
        "    for p in prefixes:\n",
        "        ids = tokenizer.encode(p + t, add_special_tokens=False)\n",
        "        if ids:\n",
        "            cands.append(ids)\n",
        "\n",
        "    # убираем дубликаты\n",
        "    uniq, seen = [], set()\n",
        "    for ids in cands:\n",
        "        key = tuple(ids)\n",
        "        if key not in seen:\n",
        "            seen.add(key)\n",
        "            uniq.append(ids)\n",
        "    return uniq\n",
        "\n",
        "def best_match(token_ids_stream, pos, unit_text, is_first, is_punct):\n",
        "    \"\"\"\n",
        "    Ищем такой вариант токенизации unit, который совпадёт с token_ids_stream[pos:pos+L].\n",
        "    \"\"\"\n",
        "    for ids in encode_unit_candidates(unit_text, is_first=is_first, is_punct=is_punct):\n",
        "        L = len(ids)\n",
        "        if token_ids_stream[pos:pos+L] == ids:\n",
        "            return ids, L, \"EXACT\"\n",
        "    return None, 0, \"NO_MATCH\"\n",
        "\n",
        "def align_units_to_tokens(token_df: pd.DataFrame, units: list):\n",
        "    \"\"\"\n",
        "    token_df: Text_X_probabilities.csv (нужны token_id, surprisal_ln)\n",
        "    units: список из extract_cell_units_from_sheet\n",
        "    Выход: unit_df (по единице-ячейке) + диагностика\n",
        "    \"\"\"\n",
        "    if \"token_id\" not in token_df.columns or \"surprisal_ln\" not in token_df.columns:\n",
        "        raise ValueError(\"В token_df нужны столбцы token_id и surprisal_ln\")\n",
        "\n",
        "    token_ids = token_df[\"token_id\"].tolist()\n",
        "    pos = 0\n",
        "\n",
        "    rows = []\n",
        "    first_mismatch = None\n",
        "\n",
        "    for ui, unit in enumerate(units):\n",
        "        is_first = (ui == 0)\n",
        "        ids, L, status = best_match(\n",
        "            token_ids_stream=token_ids,\n",
        "            pos=pos,\n",
        "            unit_text=unit[\"text\"],\n",
        "            is_first=is_first,\n",
        "            is_punct=unit[\"is_punct\"]\n",
        "        )\n",
        "\n",
        "        if L == 0:\n",
        "            if first_mismatch is None:\n",
        "                first_mismatch = {\n",
        "                    \"unit_index\": ui,\n",
        "                    \"unit_text\": unit[\"text\"],\n",
        "                    \"row\": unit[\"row\"],\n",
        "                    \"col\": unit[\"col\"],\n",
        "                    \"token_pos\": pos,\n",
        "                    \"next_token_ids\": token_ids[pos:pos+10],\n",
        "                }\n",
        "            rows.append({\n",
        "                \"unit_index\": ui,\n",
        "                \"cell_text\": unit[\"text\"],\n",
        "                \"is_punct_cell\": unit[\"is_punct\"],\n",
        "                \"row\": unit[\"row\"],\n",
        "                \"col\": unit[\"col\"],\n",
        "                \"tokens_in_unit\": None,\n",
        "                \"unit_surprisal_ln_mean\": None,\n",
        "                \"status\": \"NO_MATCH\"\n",
        "            })\n",
        "            # Важно: при NO_MATCH лучше не \"угадывать\" и не сдвигать,\n",
        "            # иначе поедет всё остальное. Остановимся.\n",
        "            break\n",
        "\n",
        "        chunk = token_df.iloc[pos:pos+L]\n",
        "        pos += L\n",
        "\n",
        "        mean_s = float(chunk[\"surprisal_ln\"].mean())\n",
        "\n",
        "        rows.append({\n",
        "            \"unit_index\": ui,\n",
        "            \"cell_text\": unit[\"text\"],\n",
        "            \"is_punct_cell\": unit[\"is_punct\"],\n",
        "            \"row\": unit[\"row\"],\n",
        "            \"col\": unit[\"col\"],\n",
        "            \"tokens_in_unit\": int(L),\n",
        "            \"unit_surprisal_ln_mean\": mean_s,\n",
        "            \"status\": \"OK\"\n",
        "        })\n",
        "\n",
        "    leftover = len(token_df) - pos\n",
        "    diag = {\n",
        "        \"units_total\": len(units),\n",
        "        \"units_aligned_ok\": sum(1 for r in rows if r[\"status\"] == \"OK\"),\n",
        "        \"tokens_total\": len(token_df),\n",
        "        \"tokens_consumed\": pos,\n",
        "        \"tokens_leftover\": leftover,\n",
        "        \"first_mismatch\": first_mismatch\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(rows), diag\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B\", use_fast=True)\n",
        "    print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x4dHsux-_36",
        "outputId": "cf7723f8-e91d-4533-d4c9-664c00364b35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocab size: 151643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# БЛОК 3: обработать тексты порциями + сохранить CSV + печать диагностики\n",
        "import os, zipfile\n",
        "\n",
        "ZIP_PATH   = \"/mnt/data/Qwen1.5_ALL-RESULTS.zip\"\n",
        "EXCEL_PATH = \"/mnt/data/data_exp.xlsx\"\n",
        "OUT_DIR    = \"/mnt/data/wordlevel_out_cells\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def process_texts_cells(text_numbers):\n",
        "    wb = openpyxl.load_workbook(EXCEL_PATH, data_only=True)\n",
        "\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "        for i in text_numbers:\n",
        "            sheet = f\"Text_{i}\"\n",
        "            csv_in_zip = f\"Qwen1.5_ALL-RESULTS/Text_{i}_probabilities.csv\"\n",
        "\n",
        "            if sheet not in wb.sheetnames:\n",
        "                print(\"[skip] no sheet:\", sheet)\n",
        "                continue\n",
        "            if csv_in_zip not in zf.namelist():\n",
        "                print(\"[skip] no csv:\", csv_in_zip)\n",
        "                continue\n",
        "\n",
        "            units = extract_cell_units_from_sheet(wb[sheet])\n",
        "\n",
        "            with zf.open(csv_in_zip) as f:\n",
        "                token_df = pd.read_csv(f)\n",
        "\n",
        "            unit_df, diag = align_units_to_tokens(token_df, units)\n",
        "\n",
        "            out_csv = os.path.join(OUT_DIR, f\"{sheet}_cell_units_wordlevel.csv\")\n",
        "            unit_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "            print(f\"\\n[{sheet}] saved -> {out_csv}\")\n",
        "            print(\"  units_total:\", diag[\"units_total\"])\n",
        "            print(\"  units_aligned_ok:\", diag[\"units_aligned_ok\"])\n",
        "            print(\"  tokens_leftover:\", diag[\"tokens_leftover\"])\n",
        "\n",
        "            # Проверка \"всё сошлось\"\n",
        "            if diag[\"units_aligned_ok\"] != diag[\"units_total\"] or diag[\"tokens_leftover\"] != 0:\n",
        "                print(\"  ⚠️ НЕ СОШЛОСЬ (alignment issue)\")\n",
        "                if diag[\"first_mismatch\"] is not None:\n",
        "                    fm = diag[\"first_mismatch\"]\n",
        "                    print(\"  first mismatch at unit:\", fm[\"unit_index\"], \"text:\", repr(fm[\"unit_text\"]))\n",
        "                    print(\"  cell:\", (fm[\"row\"], fm[\"col\"]), \"token_pos:\", fm[\"token_pos\"])\n",
        "                    print(\"  next token_ids (10):\", fm[\"next_token_ids\"])\n",
        "            else:\n",
        "                print(\"  ✅ СОШЛОСЬ: все ячейки сопоставлены и токенов не осталось\")\n",
        "\n",
        "# пример: по 2 текста\n",
        "process_texts_cells([1, 2])\n"
      ],
      "metadata": {
        "id": "P7aZ9in59SuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# БЛОК 4: собрать все Text_i_cell_units_wordlevel.csv в один Excel и скачать (Colab)\n",
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "xlsx_path = os.path.join(OUT_DIR, \"Qwen_cell_units_wordlevel.xlsx\")\n",
        "files = sorted(glob.glob(os.path.join(OUT_DIR, \"Text_*_cell_units_wordlevel.csv\")))\n",
        "\n",
        "with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
        "    for fp in files:\n",
        "        sheet = os.path.basename(fp).replace(\"_cell_units_wordlevel.csv\", \"\")\n",
        "        df = pd.read_csv(fp)\n",
        "        df.to_excel(writer, sheet_name=sheet[:31], index=False)\n",
        "\n",
        "print(\"Excel saved:\", xlsx_path)\n",
        "\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    colab_files.download(xlsx_path)\n",
        "except Exception:\n",
        "    print(\"Если не Colab — файл лежит тут:\", xlsx_path)\n"
      ],
      "metadata": {
        "id": "-mBehdzW9Xcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}