{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN93qEgiV5Vg2NqV6w2hwip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumata-arc/Probabilities_Surprisal/blob/main/Token_word.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tPvU7rh2Zxa"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# БЛОК 1: Импорт библиотек и настройки путей\n",
        "# =========================\n",
        "import os\n",
        "import math\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "ZIP_PATH = \"/mnt/data/Qwen1.5_ALL-RESULTS.zip\"\n",
        "EXCEL_PATH = \"/mnt/data/data_exp.xlsx\"\n",
        "OUT_DIR = \"/mnt/data/wordlevel_out\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# ВАЖНО: должен совпадать с тем, что ты использовал при расчёте вероятностей.\n",
        "# Если у тебя другой чекпойнт/путь — поменяй.\n",
        "TOKENIZER_NAME_OR_PATH = \"Qwen/Qwen1.5-0.5B\"\n",
        "\n",
        "# Режим \"мягкого сдвига\" при NO_MATCH:\n",
        "# True  -> если слово не сматчилось, сдвигаем pos на 1 токен и идём дальше (лучше для \"не падать\")\n",
        "# False -> если слово не сматчилось, сразу останавливаем обработку текста (лучше для строгой диагностики)\n",
        "SOFT_SHIFT_ON_NO_MATCH = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 2: Загружаем токенайзер (модель НЕ нужна)\n",
        "# =========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME_OR_PATH, use_fast=True)\n",
        "print(\"Tokenizer loaded:\", TOKENIZER_NAME_OR_PATH)\n"
      ],
      "metadata": {
        "id": "BfF0AaZI2kRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 3: Вспомогательные функции\n",
        "# - извлечение слов из листа Excel (колонки=предложения; читаем слева направо, внутри сверху вниз)\n",
        "# - кодирование слова в токены (кандидаты с/без ведущего пробела)\n",
        "# - матчинг кандидата с реальным потоком token_id из CSV\n",
        "# =========================\n",
        "def extract_words_from_sheet(ws, start_row=2, start_col=1):\n",
        "    \"\"\"Возвращает список слов в порядке чтения (continuous stream).\"\"\"\n",
        "    words = []\n",
        "    for col in range(start_col, ws.max_column + 1):\n",
        "        header = ws.cell(row=1, column=col).value\n",
        "        if header is None:\n",
        "            continue\n",
        "        for row in range(start_row, ws.max_row + 1):\n",
        "            v = ws.cell(row=row, column=col).value\n",
        "            if v is None:\n",
        "                break\n",
        "            s = str(v).strip()\n",
        "            if s == \"\":\n",
        "                break\n",
        "            words.append(s)\n",
        "    return words\n",
        "\n",
        "\n",
        "def encode_word_candidates(word, is_first):\n",
        "    \"\"\"\n",
        "    Возвращает несколько вариантов токенизации слова.\n",
        "    Термины:\n",
        "    - leading-space convention (ведущий пробел): многие BPE-токенизаторы кодируют пробел как часть следующего токена.\n",
        "    \"\"\"\n",
        "    w = str(word)\n",
        "\n",
        "    cands = []\n",
        "    # \"ожидаемое\" поведение: первое слово без пробела, остальные с ведущим пробелом\n",
        "    if is_first:\n",
        "        cands.append(tokenizer.encode(w, add_special_tokens=False))\n",
        "    else:\n",
        "        cands.append(tokenizer.encode(\" \" + w, add_special_tokens=False))\n",
        "\n",
        "    # альтернативы на случай отличий в сборке текста\n",
        "    cands.append(tokenizer.encode(w, add_special_tokens=False))\n",
        "    cands.append(tokenizer.encode(\"  \" + w, add_special_tokens=False))  # двойной пробел (редко, но безопасно проверить)\n",
        "    cands.append(tokenizer.encode(\"\\n\" + w, add_special_tokens=False))  # если где-то был перенос\n",
        "\n",
        "    # убрать дубликаты\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for ids in cands:\n",
        "        t = tuple(ids)\n",
        "        if t not in seen and len(ids) > 0:\n",
        "            seen.add(t)\n",
        "            uniq.append(ids)\n",
        "    return uniq\n",
        "\n",
        "\n",
        "def best_match_token_ids(token_ids_stream, pos, word, is_first):\n",
        "    \"\"\"\n",
        "    Смотрит: совпадает ли какой-то вариант токенизации слова с token_ids_stream[pos:pos+L]\n",
        "    Возвращает (ids, L) или (None, 0)\n",
        "    \"\"\"\n",
        "    for ids in encode_word_candidates(word, is_first):\n",
        "        L = len(ids)\n",
        "        if token_ids_stream[pos:pos+L] == ids:\n",
        "            return ids, L\n",
        "    return None, 0\n"
      ],
      "metadata": {
        "id": "nZYfds9H2nj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 4: Преобразование token-level -> word-level (continuous stream)\n",
        "# На вход: token_df (Text_X_probabilities.csv), words (из Excel)\n",
        "# На выход: word_df (одно слово = одна строка) + meta\n",
        "#\n",
        "# Главная метрика:\n",
        "# - word_surprisal_ln_sum (sum of surprisal): сумма surprisal по токенам слова (психолингвистически корректнее)\n",
        "# Доп. метрики:\n",
        "# - word_surprisal_ln_mean (mean surprisal): нормировка на количество токенов\n",
        "# - word_prob_equiv = exp(sum_logprob): условный эквивалент \"вероятности слова\"\n",
        "# =========================\n",
        "def build_wordlevel_for_text(text_id, title, token_df, words, soft_shift=True):\n",
        "    required = {\"token_id\", \"log_probability\", \"surprisal_ln\", \"surprisal_log2\"}\n",
        "    missing = required - set(token_df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"token_df missing required columns: {missing}\")\n",
        "\n",
        "    token_ids = token_df[\"token_id\"].tolist()\n",
        "    pos = 0\n",
        "\n",
        "    out_rows = []\n",
        "    ok_words = 0\n",
        "    nomatch_words = 0\n",
        "\n",
        "    for wi, word in enumerate(words):\n",
        "        is_first = (wi == 0)\n",
        "        _, L = best_match_token_ids(token_ids, pos, word, is_first)\n",
        "\n",
        "        if L == 0:\n",
        "            nomatch_words += 1\n",
        "            out_rows.append({\n",
        "                \"text_id\": text_id,\n",
        "                \"title\": title,\n",
        "                \"word_index\": wi,\n",
        "                \"word\": word,\n",
        "                \"tokens_in_word\": None,\n",
        "                \"word_logprob_sum\": None,\n",
        "                \"word_surprisal_ln_sum\": None,\n",
        "                \"word_surprisal_log2_sum\": None,\n",
        "                \"word_surprisal_ln_mean\": None,\n",
        "                \"word_prob_equiv\": None,\n",
        "                \"status\": f\"NO_MATCH at token_pos={pos}\"\n",
        "            })\n",
        "\n",
        "            if soft_shift:\n",
        "                pos += 1\n",
        "                if pos >= len(token_df):\n",
        "                    break\n",
        "                continue\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        chunk = token_df.iloc[pos:pos+L]\n",
        "        pos += L\n",
        "\n",
        "        logprob_sum = float(chunk[\"log_probability\"].sum())\n",
        "        surprisal_ln_sum = float(chunk[\"surprisal_ln\"].sum())\n",
        "        surprisal_log2_sum = float(chunk[\"surprisal_log2\"].sum())\n",
        "\n",
        "        out_rows.append({\n",
        "            \"text_id\": text_id,\n",
        "            \"title\": title,\n",
        "            \"word_index\": wi,\n",
        "            \"word\": word,\n",
        "            \"tokens_in_word\": int(L),\n",
        "            \"word_logprob_sum\": logprob_sum,\n",
        "            \"word_surprisal_ln_sum\": surprisal_ln_sum,\n",
        "            \"word_surprisal_log2_sum\": surprisal_log2_sum,\n",
        "            \"word_surprisal_ln_mean\": (surprisal_ln_sum / L) if L else None,\n",
        "            \"word_prob_equiv\": math.exp(logprob_sum),\n",
        "            \"status\": \"OK\"\n",
        "        })\n",
        "        ok_words += 1\n",
        "\n",
        "    leftover_tokens = len(token_df) - pos\n",
        "\n",
        "    word_df = pd.DataFrame(out_rows)\n",
        "    meta = {\n",
        "        \"text_id\": text_id,\n",
        "        \"title\": title,\n",
        "        \"words_total_from_excel\": len(words),\n",
        "        \"words_ok\": ok_words,\n",
        "        \"words_nomatch\": nomatch_words,\n",
        "        \"tokens_total_from_csv\": len(token_df),\n",
        "        \"tokens_consumed\": pos,\n",
        "        \"tokens_leftover\": leftover_tokens,\n",
        "        \"match_rate_words\": (ok_words / max(1, ok_words + nomatch_words)),\n",
        "    }\n",
        "    return word_df, meta\n"
      ],
      "metadata": {
        "id": "5HwslCCz2q1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 5: Агрегация word-level -> text-level summary\n",
        "# Делает сводку по тексту:\n",
        "# - сумма/среднее surprisal по словам (OK только)\n",
        "# - суммарное число токенов в словах\n",
        "# - доля матчей, остаток токенов\n",
        "# =========================\n",
        "def wordlevel_to_textsummary(word_df, meta):\n",
        "    ok = word_df[word_df[\"status\"] == \"OK\"].copy()\n",
        "    ok[\"tokens_in_word\"] = ok[\"tokens_in_word\"].astype(int)\n",
        "\n",
        "    # если вдруг нет OK-слов\n",
        "    if ok.empty:\n",
        "        return {\n",
        "            **meta,\n",
        "            \"words_ok_used\": 0,\n",
        "            \"word_surprisal_ln_sum_text\": None,\n",
        "            \"word_surprisal_ln_mean_text\": None,\n",
        "            \"word_surprisal_log2_sum_text\": None,\n",
        "            \"word_prob_equiv_text\": None,\n",
        "            \"tokens_in_words_sum\": 0,\n",
        "        }\n",
        "\n",
        "    # Главные агрегаты\n",
        "    surprisal_ln_sum_text = float(ok[\"word_surprisal_ln_sum\"].sum())\n",
        "    surprisal_ln_mean_text = float(ok[\"word_surprisal_ln_sum\"].mean())  # среднее по словам\n",
        "    surprisal_log2_sum_text = float(ok[\"word_surprisal_log2_sum\"].sum())\n",
        "\n",
        "    # Эквивалент вероятности текста (как произведение токенов через сумму logprob):\n",
        "    # exp(sum logprob по словам) = exp(sum logprob по токенам, которые вошли в слова)\n",
        "    logprob_sum_text = float(ok[\"word_logprob_sum\"].sum())\n",
        "    prob_equiv_text = math.exp(logprob_sum_text)\n",
        "\n",
        "    tokens_in_words_sum = int(ok[\"tokens_in_word\"].sum())\n",
        "\n",
        "    return {\n",
        "        **meta,\n",
        "        \"words_ok_used\": int(len(ok)),\n",
        "        \"tokens_in_words_sum\": tokens_in_words_sum,\n",
        "        \"word_surprisal_ln_sum_text\": surprisal_ln_sum_text,\n",
        "        \"word_surprisal_ln_mean_text\": surprisal_ln_mean_text,\n",
        "        \"word_surprisal_log2_sum_text\": surprisal_log2_sum_text,\n",
        "        \"word_prob_equiv_text\": prob_equiv_text,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "doCcuTld2vdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 6: Функция \"обработать выбранные тексты\"\n",
        "# Ты можешь вызывать её частями: например, process_texts([1,2]) потом process_texts([3,4]) и т.д.\n",
        "#\n",
        "# Что делает:\n",
        "# - читает слова из Excel (Text_i)\n",
        "# - читает token-level CSV из ZIP (Text_i_probabilities.csv)\n",
        "# - строит word-level CSV\n",
        "# - обновляет (дополняет) общий text-level summary CSV\n",
        "# =========================\n",
        "def process_texts(text_numbers):\n",
        "    wb = openpyxl.load_workbook(EXCEL_PATH, data_only=True)\n",
        "\n",
        "    # summary файл на уровне текстов\n",
        "    summary_path = os.path.join(OUT_DIR, \"ALL_TEXTS_WORDLEVEL_TEXTSUMMARY.csv\")\n",
        "    if os.path.exists(summary_path):\n",
        "        summary_df = pd.read_csv(summary_path)\n",
        "    else:\n",
        "        summary_df = pd.DataFrame()\n",
        "\n",
        "    new_summaries = []\n",
        "\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zf:\n",
        "        for i in text_numbers:\n",
        "            sheet_name = f\"Text_{i}\"\n",
        "            csv_name = f\"Qwen1.5_ALL-RESULTS/Text_{i}_probabilities.csv\"\n",
        "\n",
        "            if sheet_name not in wb.sheetnames:\n",
        "                print(f\"[WARN] Нет листа {sheet_name} в Excel — пропускаю.\")\n",
        "                continue\n",
        "            if csv_name not in zf.namelist():\n",
        "                print(f\"[WARN] Нет файла {csv_name} в ZIP — пропускаю.\")\n",
        "                continue\n",
        "\n",
        "            ws = wb[sheet_name]\n",
        "            words = extract_words_from_sheet(ws)\n",
        "\n",
        "            with zf.open(csv_name) as f:\n",
        "                token_df = pd.read_csv(f)\n",
        "\n",
        "            title = token_df[\"title\"].iloc[0] if \"title\" in token_df.columns and len(token_df) else None\n",
        "            text_id = token_df[\"text_id\"].iloc[0] if \"text_id\" in token_df.columns and len(token_df) else sheet_name\n",
        "\n",
        "            word_df, meta = build_wordlevel_for_text(\n",
        "                text_id=text_id,\n",
        "                title=title,\n",
        "                token_df=token_df,\n",
        "                words=words,\n",
        "                soft_shift=SOFT_SHIFT_ON_NO_MATCH\n",
        "            )\n",
        "\n",
        "            # сохраняем word-level CSV\n",
        "            out_word_path = os.path.join(OUT_DIR, f\"{sheet_name}_wordlevel.csv\")\n",
        "            word_df.to_csv(out_word_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "            # делаем text-level summary\n",
        "            text_summary = wordlevel_to_textsummary(word_df, meta)\n",
        "            text_summary[\"sheet_name\"] = sheet_name\n",
        "            new_summaries.append(text_summary)\n",
        "\n",
        "            print(\n",
        "                f\"[OK] {sheet_name} -> {out_word_path}\\n\"\n",
        "                f\"     match_rate_words={text_summary['match_rate_words']:.3f} \"\n",
        "                f\"tokens_leftover={text_summary['tokens_leftover']} \"\n",
        "                f\"words_ok={text_summary['words_ok']}/{text_summary['words_total_from_excel']}\"\n",
        "            )\n",
        "\n",
        "    if not new_summaries:\n",
        "        print(\"[INFO] Ничего не обработано.\")\n",
        "        return\n",
        "\n",
        "    new_df = pd.DataFrame(new_summaries)\n",
        "\n",
        "    # обновляем summary: если текст уже был — заменяем строку; иначе добавляем\n",
        "    if summary_df.empty:\n",
        "        summary_df = new_df\n",
        "    else:\n",
        "        # ключ для уникальности\n",
        "        key_cols = [\"sheet_name\"]\n",
        "        summary_df = summary_df[~summary_df[\"sheet_name\"].isin(new_df[\"sheet_name\"])]\n",
        "        summary_df = pd.concat([summary_df, new_df], ignore_index=True)\n",
        "\n",
        "    # сортировка по номеру текста (если возможно)\n",
        "    def _extract_num(s):\n",
        "        try:\n",
        "            return int(str(s).split(\"_\")[1])\n",
        "        except:\n",
        "            return 10**9\n",
        "\n",
        "    summary_df[\"text_num\"] = summary_df[\"sheet_name\"].apply(_extract_num)\n",
        "    summary_df = summary_df.sort_values(\"text_num\").drop(columns=[\"text_num\"])\n",
        "\n",
        "    summary_df.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\n[SUMMARY UPDATED] {summary_path}\")\n",
        "    display(summary_df)\n"
      ],
      "metadata": {
        "id": "1NDWlJVi24K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# БЛОК 7: Запуск примера (постепенная обработка)\n",
        "# Меняй список как хочешь: [1,2] затем [3,4] ...\n",
        "# =========================\n",
        "process_texts([1, 2])\n"
      ],
      "metadata": {
        "id": "Ac74mO_-2_LR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}